# MirrorStrategy
MirroredStrategy to distribute training workloads 

Integrating tf.distribute.MirroredStrategy for distributing training workloads across multiple GPUs for tf.keras models.
Distributed training can be particularly very useful when we have larger datasets. Scaling the datasets is one of the best approaches to reduce training costs.
